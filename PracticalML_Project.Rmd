---
title: "Practical Machine Learning - Project"
author: "Gito"
date: "2024-11-08"
output: 
  md_document:
    variant: markdown_github
word_document: default
---

# PRACTICAL MACHINE LEARNING PREDICTION ASSIGNMENT - JHU - COURSERA

This Project is part of JHU - Coursera Project on Practical Machine Learning. Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

Data

The training data for this project are available here: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>

The test data are available here: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

# Required Result

Your submission for the Peer Review portion should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to \< 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).

# Executive Summary

Below are the model performance summary

-   Random Forest with Cross Validation results in best result with 99.3% Accuracy with 0.69% Out-Of-Sample Error

-   Random Forest without Cross Validation shows good result with 99.28% Accuracy with 0.7% Out-Of-Sample Error

-   Pure Decision Tree only 75% Accuracy with 24% Out-Of-Sample Error

Hence Random Forest and Cross Validation enhanced model prediction

# Modeling Process :

Below are the procedure to build model :

1.  Importing, Cleaning Data and Load Necessary library\
    1.1 Removing Unnecessary Variable and NAs\
    1.2 Split Training Data into 70% Training and 30% Validation

2.  Data Modelling\
    2.1 Decision Tree Model, we will use model to predict the testing data\
    2.2 Random Forest Model with Cross Validation,we will use model to predict the testing data\
    2.3 Random Forest Model without Cross validation, we will use model to predict the testing data

3.  Conclusion, Analyzing Result Based on Model Accuracy and Out-of sample errors

# Importing, Cleaning Data and Load Necessary library

-   Here we will import Training data and Testing Data from local directory, consider "NA","", and "#DIV/0!" as NA strings

-   Split Training data to 70% Training and 30% Validation data and save the variable to train and validate variable respectively

-   testing data to be saved as testing variable

-   We can see variables/features reduced significantly after cleaning unnecessary variables, from 160 to 60 features only without reducing modelling prediction

-   Datas are then ready to be modelled

```{r }
library(caret)
library(tidyverse)
library(dplyr)
library(GGally)
library(rpart)
library(randomForest)
# library(cor)
rawdata_training <- read.csv("pml-training.csv",header = TRUE,na.strings = c("NA","","#DIV/0!"))
rawdata_testing <- read.csv("pml-testing.csv",header = TRUE,na.strings = c("NA","","#DIV/0!"))
dim(rawdata_training)
dim(rawdata_testing)
```

## Clean Data

By Analyzing the data, we can see many NA strings and we might need to analyze which variables are not important to the model. Below are my methodologies to remove unnecessary data 1. Low variability data 2. NA values will be removed --\> By recognizing sum of NA for each variable, if sum is 0 then its considered as NA column 3. Remove time variable, window variable, name and 1st columns

```{r }
# Removing NAs
rawdata_training <- rawdata_training[,colSums(is.na(rawdata_training)) == 0]
rawdata_testing <- rawdata_testing[,colSums(is.na(rawdata_testing)) == 0]
dim(rawdata_training)
dim(rawdata_testing)
```

```{r }
rawdata_training_clean <- rawdata_training[,8:60]
rawdata_testing_clean <- rawdata_testing[,8:60]
```

We will split our training to 70% to generate model & 30% to validate model to quantify the performance before passing to data testing set

```{r }
set.seed(123)
train_sample <- createDataPartition(rawdata_training_clean$classe,p=0.7,list = FALSE)
train <- rawdata_training_clean[train_sample,]
validate <- rawdata_training_clean[-train_sample,]
```

# Data Modelling

We will use 3 Model, Decision Tree, Random Forest with Cross Validation and Random Forest without Cross Validation

## 1. 1st Model, Basic Decision Tree

### 1.1 Decision Tree Model

We build Decision Tree model for train data and validate model using validate data and we will see the performances. and store model result as model1

We pass object classe to rpart function and we use method "class" to predict class variable instead of probability. Below is the Decision Tree chart result.

```{r }
model1 <- rpart(classe~.,data = train,method = "class")
par(cex=0.7)
plot(model1)
text(model1)
```

### 1.2 Decision Tree Model Prediction Performance

This procedure is to validate the model1 using validate sample data and measure its performance based on Confusion Matrix Indicator and Out-of-sample error.

-   Model Accuracy 75.7%

-   Out-of-sample error 24.3%

```{r }
predict_validate_tree <- predict(model1,validate,type = "class")
CF_DTree <- confusionMatrix(as.factor(validate$classe),predict_validate_tree)
CF_DTree
```

## 2. 2nd Model, Random Forest with Cross Validation

We build 2nd Model with Cross Validation for train data and validate model using validate data and we will see the performances. and store model result as model2.

We apply Cross Validation function with 10 Fold Cross Validation and 100 tree

We pass object classe to train function and we use trcontrol function to apply cross validation with 10 Fold. Below is Random Forest Chart with CV result.

### 2.1 Random Forest Model with Cross Validation (CV)

```{r }
set.seed(321)
model2 <- train(classe~.,data = train,method = "rf", trcontrol = trainControl(method = "cv",10),ntree = 100)
model2
plot(model2)
```

### 2.2 Random Forest with Cross Validation Prediction Performance

This procedure is to validate the model2 using validate sample data and measure its performance based on Confusion Matrix Indicator and Out-of-sample error.

-   Model Accuracy 99.31%

-   Out-of-sample error 0.69%

```{r }
predict_validate_rforest <- predict(model2,validate)
CF_RFCV <- confusionMatrix(as.factor(validate$classe),predict_validate_rforest)
CF_RFCV

```

## 3. 3rd Model, Random Forest without Cross Validation

We build3rd Model without Cross Validation and validate model using validate data and we will see the performances. and store model result as model3.

Since no cross validation required so we dont use trcontrol function.

### 3.1 Random Forest Model without Cross Validation

```{r }
set.seed(31)
model3 <- train(classe~.,data = train,method = "rf", ntree = 100)
model3
plot(model3)
```

### 3.2 Random Forest Prediction Performance

This procedure is to validate the model3 using validate sample data and measure its performance based on Confusion Matrix Indicator and Out-of-sample error.

-   Model Accuracy 99.29%

-   Out-of-sample error 0.7%

```{r }
predict_validate_rforest_noCV <- predict(model3,validate)
CF_RF <- confusionMatrix(as.factor(validate$classe),predict_validate_rforest_noCV)
CF_RF
```

## Performance Benchmark Chart Random Forest with Cross Validation and Without Cross validataion

```{r }
plot(model2$results[,1],model2$results[,2],type = "l",col = "blue",xlab = "Predictors",ylab = "Accuracy", main = "Random Forest with CV & Without CV Performance Benchmark")
# lines(model2$results[,1],model2$results[,2],col = "blue",xlab = "Predictors",ylab = "Accuracy", main = "Random Forest with CV & Without CV Performance Benchmark")
lines(model3$results[,1],model3$results[,2],col = "green")
legend("topright", legend = c("R Forest with CV","R Forest without CV"), col = c("blue","green"),lty = 1)
```

# CONCLUSION, Prediction on Test Data

Below are the model performance summary

-   Random Forest with Cross Validation results in best result with 99.3% Accuracy with 0.69% Out-Of-Sample Error

-   Random Forest without Cross Validation shows good result with 99.28% Accuracy with 0.7% Out-Of-Sample Error

-   Pure Decision Tree only 75% Accuracy with 24% Out-Of-Sample Error

Hence Random Forest and Cross Validation enhanced model prediction

## 1. Prediction with Test Data Result for Problematic data

```{r }
predict1_test <- predict(model1,rawdata_testing_clean[,1:52],type = "class")
predict2_test <- predict(model2,rawdata_testing_clean[,1:52])
predict3_test <- predict(model3,rawdata_testing_clean[,1:52])
print(data.frame("problem" = rawdata_testing_clean$problem_id,"Decision Tree Prediction" = predict1_test,"Random Forest with CV Prediction" = predict2_test, "Random Forest w/o CV" = predict3_test))
```

## 2. Model Accuracy Benchmark for Problematic Data

```{r }
print(data.frame("Decision Tree" = CF_DTree$overall[1], "Random Forest with CV" = CF_RFCV$overall[1],"Random Forest w/o CV" = CF_RF$overall[1]))
```
